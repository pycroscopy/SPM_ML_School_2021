{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reinforcement Learning Examples",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fW2N7-lWg7YT"
      },
      "source": [
        "# Reinforcement Learning - Discrete and Continuous action spaces\n",
        "\n",
        "R. Vasudevan\n",
        "\n",
        "Here, we will explore two different RL algorithms: \n",
        "\n",
        "1. Deep Q Learning, which is useful for discrete action spaces,\n",
        "2. Actor-Critic algorithm (A2C) which is a policy gradient method, and is good for continuous action spaces."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsgWUusyg7RJ"
      },
      "source": [
        "# 1. Deep Q Learning\n",
        "\n",
        "Here we will attempt to 'solve' the game of 'Cartpole', which is a very basic game where the goal is to keep a pole upright as it wobbles on a cart. You have two choices at each state: move the cart left or right."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iRdIMFMhYI6"
      },
      "source": [
        "#Let's import some necessary packages\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "#And then for the agent\n",
        "from tensorflow.keras import Model, layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.initializers import GlorotUniform\n",
        "import tensorflow as tf\n",
        "import gym\n",
        "import sys\n",
        "#tf.device('/cpu:0') # Put it on cpu\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSZ5FC48hb-w"
      },
      "source": [
        "## Define the necessary classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4It4nn8hYOw"
      },
      "source": [
        "# create agent class\n",
        "class Agent():\n",
        "    def __init__(self, gamma, epsilon, lr,\n",
        "                  batchSize, \n",
        "                 nActions=5, maxMem=45, epsEnd=0.01,\n",
        "                 epsDec=0.005, ddqn = False):\n",
        "        \n",
        "        self.gamma = gamma #Discount Factor\n",
        "        self.epsilon = epsilon #Epsilon, from greedy\n",
        "        self.lr = lr\n",
        "        self.frame_history = []\n",
        "        self.batchSize = batchSize\n",
        "        self.epsMin = epsEnd\n",
        "        self.epsDec = epsDec\n",
        "        self.memsize = maxMem\n",
        "        self.nActions = nActions\n",
        "        self.qvals_list = []\n",
        "        self.episode_history = []\n",
        "        self.episode_number = 0\n",
        "\n",
        "        # call and evaluate using DQN class (and make target)\n",
        "        self.q_eval = DQN_Keras(dim_actions=self.nActions, actor_lr=self.lr)\n",
        "        self.q_eval.compile()\n",
        "    \n",
        "    def choose_action(self, obsv):\n",
        "        if np.random.random() > self.epsilon:\n",
        "            qvals = self.q_eval(obsv)\n",
        "            action = [np.argmax(qvals)]\n",
        "            #print('agent action, is {}'.format(action))\n",
        "        else:\n",
        "            action = np.random.choice(np.arange(self.nActions), size=1)\n",
        "            #print('random action, is {}'.format(action))\n",
        "        self.action = action\n",
        "        return action\n",
        "\n",
        "    def store_transitions(self, transition):\n",
        "        self.episode_history.append(transition)\n",
        "        self.episode_history = self.episode_history[-self.memsize:]\n",
        "        \n",
        "    def learn(self):\n",
        "    \n",
        "        size_to_select = min(len(self.episode_history), self.batchSize)\n",
        "\n",
        "        # experience replay\n",
        "        batch = np.random.choice(np.arange(len(self.episode_history)),\n",
        "                          size_to_select, replace=False)\n",
        "\n",
        "        q_values_list, state_buffer, action_hist = [], [],[]\n",
        "        batch_history = np.array(self.episode_history, dtype = 'object')\n",
        "\n",
        "        for state, action, reward, done, next_state in batch_history[batch]:\n",
        "            \n",
        "            state_buffer.append(state)\n",
        "    \n",
        "            # qval (s,a) = reward + gamma*max(a')qval(s',a')\n",
        "            #print(state.shape)\n",
        "            qvals_initial = agent.q_eval(state)[0]\n",
        "            qvals_initial = qvals_initial.numpy()\n",
        "            q_update = reward + self.gamma * np.max(agent.q_eval(next_state)[0]) * done\n",
        "            qvals_initial[action] = q_update\n",
        "            q_update = qvals_initial\n",
        "            \n",
        "            q_values_list.append(q_update)\n",
        "            action_hist.append(action)\n",
        "                \n",
        "        q_values_list = tf.stack(q_values_list)\n",
        "        \n",
        "      \n",
        "        with tf.GradientTape() as tape:\n",
        "            q_output = tf.squeeze(tf.stack([agent.q_eval(state)for state in state_buffer]))\n",
        "            loss = tf.square(q_output - q_values_list)\n",
        "            valueGradient = tape.gradient(loss, agent.q_eval.trainable_variables)\n",
        "            \n",
        "        agent.q_eval.optimizer.apply_gradients(zip(valueGradient, agent.q_eval.trainable_variables))\n",
        "        agent.epsilon = agent.epsilon - agent.epsDec if agent.epsilon > agent.epsMin else agent.epsMin\n",
        "\n",
        "        return loss\n",
        "    \n",
        "# create DQN class\n",
        "class DQN_Keras(Model):\n",
        "    def __init__(self, input_dim=4,\n",
        "                 dim_actions=2,  num_hidden_nodes_1=128,\n",
        "                 num_hidden_nodes_2=128,\n",
        "                 actor_lr=0.001):\n",
        "        \n",
        "        self.initializer = tf.keras.initializers.he_uniform()\n",
        "        self.num_hidden_nodes_1 = num_hidden_nodes_1\n",
        "        self.num_hidden_nodes_2 = num_hidden_nodes_2\n",
        "        self.dim_actions = dim_actions\n",
        "        self.input_dim = input_dim\n",
        "        self.actor_lr = actor_lr\n",
        "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.actor_lr)\n",
        "        super(DQN_Keras, self).__init__()\n",
        "        self.actor = self.build_actor()\n",
        "        self.actor.compile(loss=tf.keras.losses.mean_squared_error, optimizer=self.optimizer,\n",
        "                           metrics=['accuracy'])\n",
        "\n",
        "    def build_actor(self):\n",
        "        \n",
        "        Input = layers.Input(shape=self.input_dim)\n",
        "        cnet = layers.Dense(self.num_hidden_nodes_1, activation='relu', \n",
        "                           kernel_initializer = self.initializer)(Input)\n",
        "        \n",
        "        cnet = layers.Dense(self.num_hidden_nodes_2, activation='relu', \n",
        "                           kernel_initializer = self.initializer)(cnet)\n",
        "        \n",
        "        cnet = layers.Dense(self.dim_actions, activation='linear',\n",
        "                                kernel_initializer=self.initializer,\n",
        "                            name='output_qvalues')(cnet)\n",
        "        \n",
        "        actor = Model(inputs = Input, outputs = cnet)\n",
        "\n",
        "        return actor\n",
        "\n",
        "    # define forward pass\n",
        "    def call(self, states):\n",
        "        actions_output = self.actor(states[None,:])\n",
        "\n",
        "        # this returns the qvalues\n",
        "        return actions_output\n",
        "    \n",
        "        "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTxOZc8xhgcQ"
      },
      "source": [
        "gamma = 0.90 #discount factor\n",
        "epsilon_start = 0.99 #start of epsilon\n",
        "epsilon_dec = 0.0003 #decay of epsilon after each learning step\n",
        "batch_size = 32 #batch size for experienc ereplay\n",
        "agent_lr = 0.0001\n",
        "\n",
        "#Make the environment\n",
        "env = gym.make('CartPole-v1')\n",
        "\n",
        "env.reset() #Reset\n",
        "\n",
        "agent = Agent(gamma=gamma,epsilon=epsilon_start, epsDec =epsilon_dec,\n",
        "              batchSize=batch_size, lr=agent_lr, \n",
        "              nActions=2)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUkyhSRAhkWc"
      },
      "source": [
        "## DQN Agent Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sC2UKFVhgeO",
        "outputId": "8fdf5ba7-d53a-4b6d-f860-8eb8a80bd43d"
      },
      "source": [
        "## DQN Agent Training\n",
        "#Here we train the agent\n",
        "NUM_EPISODES=100\n",
        "\n",
        "scores, epsHistory = [], []\n",
        " \n",
        "for ep in range(NUM_EPISODES):\n",
        "    done = False\n",
        "    score=0\n",
        "    state = env.reset()\n",
        "    \n",
        "    while not done:\n",
        "\n",
        "        action = agent.choose_action(state)[0] #Get the agent to choose the action\n",
        "        next_state, reward, done,_ = env.step(action) #Step the environment\n",
        "        \n",
        "        if done: done_val = 0\n",
        "        else: done_val = 1\n",
        "        \n",
        "        agent.store_transitions([state,action,reward,done_val,next_state]) #store the transition\n",
        "    \n",
        "        score+=reward\n",
        "        agent.learn() #update q function\n",
        "        \n",
        "        state = next_state\n",
        "        \n",
        "    scores.append(score)\n",
        "    if ep>5:\n",
        "        print('ep {} has reward {:.2f}, mean reward from last 5 episodes is {:.2f}, epsilon is {:.2f}'.format(\n",
        "        ep, score, np.mean(scores[-5:]), agent.epsilon))\n",
        "    epsHistory.append(agent.epsilon)\n",
        "    agent.episode_number+=1\n",
        "    "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ep 6 has reward 23.00, mean reward from last 5 episodes is 23.20, epsilon is 0.95\n",
            "ep 7 has reward 16.00, mean reward from last 5 episodes is 21.40, epsilon is 0.94\n",
            "ep 8 has reward 15.00, mean reward from last 5 episodes is 20.20, epsilon is 0.94\n",
            "ep 9 has reward 12.00, mean reward from last 5 episodes is 16.40, epsilon is 0.93\n",
            "ep 10 has reward 18.00, mean reward from last 5 episodes is 16.80, epsilon is 0.93\n",
            "ep 11 has reward 15.00, mean reward from last 5 episodes is 15.20, epsilon is 0.92\n",
            "ep 12 has reward 11.00, mean reward from last 5 episodes is 14.20, epsilon is 0.92\n",
            "ep 13 has reward 27.00, mean reward from last 5 episodes is 16.60, epsilon is 0.91\n",
            "ep 14 has reward 13.00, mean reward from last 5 episodes is 16.80, epsilon is 0.91\n",
            "ep 15 has reward 17.00, mean reward from last 5 episodes is 16.60, epsilon is 0.90\n",
            "ep 16 has reward 25.00, mean reward from last 5 episodes is 18.60, epsilon is 0.89\n",
            "ep 17 has reward 21.00, mean reward from last 5 episodes is 20.60, epsilon is 0.89\n",
            "ep 18 has reward 47.00, mean reward from last 5 episodes is 24.60, epsilon is 0.87\n",
            "ep 19 has reward 22.00, mean reward from last 5 episodes is 26.40, epsilon is 0.87\n",
            "ep 20 has reward 21.00, mean reward from last 5 episodes is 27.20, epsilon is 0.86\n",
            "ep 21 has reward 17.00, mean reward from last 5 episodes is 25.60, epsilon is 0.86\n",
            "ep 22 has reward 23.00, mean reward from last 5 episodes is 26.00, epsilon is 0.85\n",
            "ep 23 has reward 36.00, mean reward from last 5 episodes is 23.80, epsilon is 0.84\n",
            "ep 24 has reward 21.00, mean reward from last 5 episodes is 23.60, epsilon is 0.83\n",
            "ep 25 has reward 13.00, mean reward from last 5 episodes is 22.00, epsilon is 0.83\n",
            "ep 26 has reward 17.00, mean reward from last 5 episodes is 22.00, epsilon is 0.82\n",
            "ep 27 has reward 35.00, mean reward from last 5 episodes is 24.40, epsilon is 0.81\n",
            "ep 28 has reward 44.00, mean reward from last 5 episodes is 26.00, epsilon is 0.80\n",
            "ep 29 has reward 18.00, mean reward from last 5 episodes is 25.40, epsilon is 0.79\n",
            "ep 30 has reward 29.00, mean reward from last 5 episodes is 28.60, epsilon is 0.79\n",
            "ep 31 has reward 41.00, mean reward from last 5 episodes is 33.40, epsilon is 0.77\n",
            "ep 32 has reward 15.00, mean reward from last 5 episodes is 29.40, epsilon is 0.77\n",
            "ep 33 has reward 164.00, mean reward from last 5 episodes is 53.40, epsilon is 0.72\n",
            "ep 34 has reward 58.00, mean reward from last 5 episodes is 61.40, epsilon is 0.70\n",
            "ep 35 has reward 13.00, mean reward from last 5 episodes is 58.20, epsilon is 0.70\n",
            "ep 36 has reward 14.00, mean reward from last 5 episodes is 52.80, epsilon is 0.69\n",
            "ep 37 has reward 35.00, mean reward from last 5 episodes is 56.80, epsilon is 0.68\n",
            "ep 38 has reward 14.00, mean reward from last 5 episodes is 26.80, epsilon is 0.68\n",
            "ep 39 has reward 16.00, mean reward from last 5 episodes is 18.40, epsilon is 0.67\n",
            "ep 40 has reward 10.00, mean reward from last 5 episodes is 17.80, epsilon is 0.67\n",
            "ep 41 has reward 9.00, mean reward from last 5 episodes is 16.80, epsilon is 0.67\n",
            "ep 42 has reward 14.00, mean reward from last 5 episodes is 12.60, epsilon is 0.66\n",
            "ep 43 has reward 22.00, mean reward from last 5 episodes is 14.20, epsilon is 0.66\n",
            "ep 44 has reward 26.00, mean reward from last 5 episodes is 16.20, epsilon is 0.65\n",
            "ep 45 has reward 17.00, mean reward from last 5 episodes is 17.60, epsilon is 0.64\n",
            "ep 46 has reward 16.00, mean reward from last 5 episodes is 19.00, epsilon is 0.64\n",
            "ep 47 has reward 19.00, mean reward from last 5 episodes is 20.00, epsilon is 0.63\n",
            "ep 48 has reward 28.00, mean reward from last 5 episodes is 21.20, epsilon is 0.63\n",
            "ep 49 has reward 35.00, mean reward from last 5 episodes is 23.00, epsilon is 0.62\n",
            "ep 50 has reward 97.00, mean reward from last 5 episodes is 39.00, epsilon is 0.59\n",
            "ep 51 has reward 27.00, mean reward from last 5 episodes is 41.20, epsilon is 0.58\n",
            "ep 52 has reward 24.00, mean reward from last 5 episodes is 42.20, epsilon is 0.57\n",
            "ep 53 has reward 17.00, mean reward from last 5 episodes is 40.00, epsilon is 0.57\n",
            "ep 54 has reward 16.00, mean reward from last 5 episodes is 36.20, epsilon is 0.56\n",
            "ep 55 has reward 20.00, mean reward from last 5 episodes is 20.80, epsilon is 0.56\n",
            "ep 56 has reward 40.00, mean reward from last 5 episodes is 23.40, epsilon is 0.54\n",
            "ep 57 has reward 27.00, mean reward from last 5 episodes is 24.00, epsilon is 0.53\n",
            "ep 58 has reward 60.00, mean reward from last 5 episodes is 32.60, epsilon is 0.52\n",
            "ep 59 has reward 95.00, mean reward from last 5 episodes is 48.40, epsilon is 0.49\n",
            "ep 60 has reward 36.00, mean reward from last 5 episodes is 51.60, epsilon is 0.48\n",
            "ep 61 has reward 79.00, mean reward from last 5 episodes is 59.40, epsilon is 0.45\n",
            "ep 62 has reward 53.00, mean reward from last 5 episodes is 64.60, epsilon is 0.44\n",
            "ep 63 has reward 68.00, mean reward from last 5 episodes is 66.20, epsilon is 0.42\n",
            "ep 64 has reward 45.00, mean reward from last 5 episodes is 56.20, epsilon is 0.40\n",
            "ep 65 has reward 57.00, mean reward from last 5 episodes is 60.40, epsilon is 0.39\n",
            "ep 66 has reward 91.00, mean reward from last 5 episodes is 62.80, epsilon is 0.36\n",
            "ep 67 has reward 58.00, mean reward from last 5 episodes is 63.80, epsilon is 0.34\n",
            "ep 68 has reward 33.00, mean reward from last 5 episodes is 56.80, epsilon is 0.33\n",
            "ep 69 has reward 37.00, mean reward from last 5 episodes is 55.20, epsilon is 0.32\n",
            "ep 70 has reward 32.00, mean reward from last 5 episodes is 50.20, epsilon is 0.31\n",
            "ep 71 has reward 21.00, mean reward from last 5 episodes is 36.20, epsilon is 0.31\n",
            "ep 72 has reward 19.00, mean reward from last 5 episodes is 28.40, epsilon is 0.30\n",
            "ep 73 has reward 27.00, mean reward from last 5 episodes is 27.20, epsilon is 0.29\n",
            "ep 74 has reward 27.00, mean reward from last 5 episodes is 25.20, epsilon is 0.28\n",
            "ep 75 has reward 21.00, mean reward from last 5 episodes is 23.00, epsilon is 0.28\n",
            "ep 76 has reward 110.00, mean reward from last 5 episodes is 40.80, epsilon is 0.24\n",
            "ep 77 has reward 143.00, mean reward from last 5 episodes is 65.60, epsilon is 0.20\n",
            "ep 78 has reward 71.00, mean reward from last 5 episodes is 74.40, epsilon is 0.18\n",
            "ep 79 has reward 19.00, mean reward from last 5 episodes is 72.80, epsilon is 0.17\n",
            "ep 80 has reward 53.00, mean reward from last 5 episodes is 79.20, epsilon is 0.16\n",
            "ep 81 has reward 26.00, mean reward from last 5 episodes is 62.40, epsilon is 0.15\n",
            "ep 82 has reward 32.00, mean reward from last 5 episodes is 40.20, epsilon is 0.14\n",
            "ep 83 has reward 25.00, mean reward from last 5 episodes is 31.00, epsilon is 0.13\n",
            "ep 84 has reward 23.00, mean reward from last 5 episodes is 31.80, epsilon is 0.13\n",
            "ep 85 has reward 20.00, mean reward from last 5 episodes is 25.20, epsilon is 0.12\n",
            "ep 86 has reward 29.00, mean reward from last 5 episodes is 25.80, epsilon is 0.11\n",
            "ep 87 has reward 56.00, mean reward from last 5 episodes is 30.60, epsilon is 0.10\n",
            "ep 88 has reward 85.00, mean reward from last 5 episodes is 42.60, epsilon is 0.07\n",
            "ep 89 has reward 264.00, mean reward from last 5 episodes is 90.80, epsilon is 0.01\n",
            "ep 90 has reward 129.00, mean reward from last 5 episodes is 112.60, epsilon is 0.01\n",
            "ep 91 has reward 277.00, mean reward from last 5 episodes is 162.20, epsilon is 0.01\n",
            "ep 92 has reward 168.00, mean reward from last 5 episodes is 184.60, epsilon is 0.01\n",
            "ep 93 has reward 133.00, mean reward from last 5 episodes is 194.20, epsilon is 0.01\n",
            "ep 94 has reward 213.00, mean reward from last 5 episodes is 184.00, epsilon is 0.01\n",
            "ep 95 has reward 256.00, mean reward from last 5 episodes is 209.40, epsilon is 0.01\n",
            "ep 96 has reward 500.00, mean reward from last 5 episodes is 254.00, epsilon is 0.01\n",
            "ep 97 has reward 60.00, mean reward from last 5 episodes is 232.40, epsilon is 0.01\n",
            "ep 98 has reward 500.00, mean reward from last 5 episodes is 305.80, epsilon is 0.01\n",
            "ep 99 has reward 10.00, mean reward from last 5 episodes is 265.20, epsilon is 0.01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeCy1RPHhxDY"
      },
      "source": [
        "## Plot Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-BoVbBHhvZw",
        "outputId": "daf08d6c-61de-4fb1-a27a-342aa8912d02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "# Let's plot the results\n",
        "%matplotlib notebook\n",
        "def smooth_window(data, window_size):\n",
        "    return np.convolve(data, np.ones((window_size,))/window_size, mode='valid')\n",
        " \n",
        "plt.figure()\n",
        "plt.plot(smooth_window(scores,5))\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Average Reward')\n",
        "ax2 = plt.twinx()\n",
        "ax2.plot(smooth_window(epsHistory,5), 'r--')\n",
        "ax2.set_ylabel('Epsilon')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a71f758e3f1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmooth_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Episode'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pax68QmDiBI8"
      },
      "source": [
        "### Exercise: \n",
        "1. See how the training changes when you reduce the size of the network\n",
        "2.  Can you now move to a different environment, like the MountainCar example, and see if you can train on that environment?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wy4OVD4hzbI"
      },
      "source": [
        "# 2. Actor-Critic Algorithm\n",
        "\n",
        "Here we will now employ an actor-critic algorithm to tackle a continuous action space task - the classic 'pendulum swing'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzYuEgXmozgQ"
      },
      "source": [
        "import sys\n",
        "import gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "import tensorflow_probability as tfp\n",
        "from collections import namedtuple\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "#And then for the agent\n",
        "from tensorflow.keras import Model, layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.initializers import GlorotUniform\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "EPISODES = 500\n",
        "\n",
        "# A2C(Advantage Actor-Critic) agent for Pendulum\n",
        "class CL_NeuralNet_A2C(Model):\n",
        "  # This defines two distinct models\n",
        "  # One is an actor, another is the critic (value function estimation)\n",
        "  # Both are fully connected neural networks\n",
        "\n",
        "    def __init__(self, input_dim=3, dim_actions=1, num_hidden_nodes_1=128,\n",
        "                 num_hidden_nodes_2 = 64,\n",
        "                actor_lr=0.0002, critic_lr=0.0005, gamma = 0.90):\n",
        "        self.num_hidden_nodes_1 = num_hidden_nodes_1\n",
        "        self.num_hidden_nodes_2 = num_hidden_nodes_2\n",
        "        self.input_dim = input_dim\n",
        "        self.dim_actions = dim_actions\n",
        "        self.critic_lr = critic_lr\n",
        "        self.actor_lr = actor_lr\n",
        "        self.initializer = tf.keras.initializers.glorot_uniform()\n",
        "        super(CL_NeuralNet_A2C, self).__init__()\n",
        "        self.actor = self.build_actor()\n",
        "        self.critic = self.build_critic()\n",
        "        self.actor.optimizer = tf.keras.optimizers.Adam(learning_rate=self.actor_lr)\n",
        "        self.critic.optimizer = tf.keras.optimizers.Adam(learning_rate=self.critic_lr)\n",
        "        self.transitions = []\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def build_actor(self):\n",
        "        actor = Sequential()\n",
        "        actor.add(layers.Input(shape=self.input_dim))\n",
        "        actor.add(layers.Dense(self.num_hidden_nodes_1, activation=tf.nn.relu,\n",
        "                              kernel_initializer=self.initializer,\n",
        "                              name='fc_1'))\n",
        "        actor.add(layers.Dense(self.num_hidden_nodes_2, activation=tf.nn.relu,\n",
        "                              kernel_initializer=self.initializer,\n",
        "                              name='fc_2'))\n",
        "        actor.add(layers.Dense(self.dim_actions * 2, activation='linear',\n",
        "                              kernel_initializer=self.initializer,\n",
        "                              name='output_actions_layer'))\n",
        "\n",
        "      # here the actor will output a mean, standard deviation from which we will sample.\n",
        "\n",
        "        return actor\n",
        "\n",
        "    def build_critic(self):\n",
        "        # critic neural network\n",
        "        critic = Sequential()\n",
        "\n",
        "        critic.add(layers.Dense(self.num_hidden_nodes_1, activation=tf.nn.relu,\n",
        "                              kernel_initializer=self.initializer , input_shape=(self.input_dim,),\n",
        "                              name='fcc_1'))\n",
        "        critic.add(layers.Dense(self.num_hidden_nodes_2, activation=tf.nn.relu,\n",
        "                              kernel_initializer=self.initializer , input_shape=(self.input_dim,),\n",
        "                              name='fc_2'))\n",
        "        critic.add(layers.Dense(1, activation='linear',\n",
        "                              kernel_initializer=self.initializer ,\n",
        "                              name='output_actions_layer_critic'))\n",
        "\n",
        "        return critic\n",
        "\n",
        "    # define forward pass\n",
        "    def call(self, states):\n",
        "        actions_output = self.actor(states)\n",
        "        value_estimate = self.critic(states)\n",
        "        return actions_output, value_estimate\n",
        "    \n",
        "    def store(self, transition_tuple):\n",
        "      self.transitions.append(transition_tuple)\n",
        "\n",
        "    def learn(self):\n",
        "        discounted_rs, states, actions, done_vals = [] ,[] , [] , []\n",
        "        for ind in range(len(self.transitions)):\n",
        "            state, action,reward, done, next_state = self.transitions[ind]\n",
        "            if done is True: done_val = 0\n",
        "            else: done_val = 1  \n",
        "            done_vals.append(done_val)\n",
        "            discounted_r = reward + self.gamma*self.critic(next_state[None,:])*done_val\n",
        "            discounted_rs.append(discounted_r)\n",
        "            states.append(state)\n",
        "            actions.append(action)\n",
        "\n",
        "        states_tf = tf.stack(states)\n",
        "        state_values = self.critic(states_tf)\n",
        "        advantage_estimate = np.array(discounted_rs) - state_values.numpy()\n",
        "        actions_tf = tf.stack(actions)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Calculate the policy gradient\n",
        "            actions_mean_tt = tf.reshape(self.actor(states_tf), (-1, 2))\n",
        "            lognorm_dist = tfp.distributions.MultivariateNormalDiag(\n",
        "                actions_mean_tt[:, 0],\n",
        "                tf.nn.softplus(actions_mean_tt[:, 1])).log_prob(actions_tf)\n",
        "            loss = -tf.reduce_mean(lognorm_dist * advantage_estimate)  # gradient of objective function\n",
        "            gradients = tape.gradient(loss, self.actor.trainable_variables)\n",
        "        self.actor.optimizer.apply_gradients(zip(gradients, self.actor.trainable_variables))\n",
        "\n",
        "        # Critic loss\n",
        "        with tf.GradientTape() as tape:\n",
        "            state_values = self.critic(states_tf)\n",
        "            state_values = state_values * done_vals\n",
        "            td_error = tf.reduce_mean((np.squeeze(np.array(discounted_rs)) - state_values) ** 2)  # TD_error\n",
        "            valueGradient = tape.gradient(td_error, policy.critic.trainable_variables)\n",
        "            \n",
        "        self.critic.optimizer.apply_gradients(zip(valueGradient, policy.critic.trainable_variables))\n",
        "        self.transitions = []\n",
        "\n",
        "        return loss, td_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTeI-LNsg5Z2"
      },
      "source": [
        "## Agent Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJuiRO75o1Q3"
      },
      "source": [
        "\n",
        "env = gym.make('Pendulum-v0')\n",
        "\n",
        "#The policy is a global variable. There will be one policy per MPI process\n",
        "policy =  CL_NeuralNet_A2C(actor_lr=0.0001, critic_lr=0.0002, input_dim=3, dim_actions = 1)\n",
        "\n",
        "train_int = 64\n",
        "scores_total = []\n",
        "\n",
        "for ep in range(EPISODES):\n",
        "    done = False\n",
        "    state = env.reset()\n",
        "    score = 0\n",
        "    step = 0\n",
        "    while not done:\n",
        "    \n",
        "        actions_output, value_estimate = policy(state[None,:])\n",
        "        # actions are given by mu, sigma (action_dims x 2) tensor\n",
        "        \n",
        "        step +=1\n",
        "\n",
        "        # Sample the policy to get the action\n",
        "        output_action = tfp.distributions.MultivariateNormalDiag(\n",
        "            actions_output[ :, 0], tf.nn.softplus(actions_output[:, 1]) + 1E-2).sample(1)\n",
        "\n",
        "        action = np.squeeze(tf.clip_by_value(output_action, -2, 2).numpy())\n",
        "\n",
        "        # Take the selected action in the environment\n",
        "        next_state, reward, done, _ = env.step([action])\n",
        "\n",
        "        if step==32: done = True\n",
        "\n",
        "        Transition = [state, action,  reward, done, next_state]\n",
        "\n",
        "        policy.store(Transition)\n",
        "\n",
        "        state = next_state\n",
        "        score+=reward\n",
        "\n",
        "    policy.learn()\n",
        "\n",
        "    if ep==0:\n",
        "      scores_total.append(score)\n",
        "    else:\n",
        "      scores_total.append(0.1*score + 0.9*scores_total[-1])\n",
        "\n",
        "    if ep>=2:\n",
        "      print(\"ep {} with reward {} and average score {}\".format(ep, score, scores_total[-1]))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QbOy6BcpVSE"
      },
      "source": [
        "\n",
        "def smooth_window(data, window_size):\n",
        "    return np.convolve(data, np.ones((window_size,))/window_size, mode='valid')\n",
        " \n",
        "plt.figure()\n",
        "plt.plot(smooth_window(scores_total,20))\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Average Reward')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWcKUotJyq4s"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}